{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nYV87aO_-TY1"},"outputs":[],"source":["#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UW6E36Bbgi8g","outputId":"9a2c05a9-828a-488b-a44c-2b286cd02620","executionInfo":{"status":"ok","timestamp":1641882897184,"user_tz":-420,"elapsed":18469,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVNa6Acd-bn8","outputId":"61fba343-2638-4230-9985-bc2d7b369c86","executionInfo":{"status":"ok","timestamp":1641886448267,"user_tz":-420,"elapsed":3751,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abBpymzdMAjb"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","from transformers import AutoModel, AutoTokenizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import json\n","from tqdm import tqdm\n","from torch.nn.functional import one_hot"]},{"cell_type":"markdown","metadata":{"id":"S-NTNlMo-2Bw"},"source":["#Data preprocess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5O7EQBwUh9lY"},"outputs":[],"source":["# xquad_vi_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/_train_xquad_vi.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKm6dH0m0MkO"},"outputs":[],"source":["# mlqa_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/mlqa_trainingdata.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDcwUSZm0XUa"},"outputs":[],"source":["# # convert mlqa format to zalo format\n","# datas = []\n","# i = 0\n","# for data in mlqa_df['data']:\n","#   for paragraph in data['paragraphs']:\n","#     for qas in paragraph['qas']:\n","#       temp_dict = {}\n","#       temp_dict['id'] = qas['id']\n","#       temp_dict['question'] = qas['question']\n","#       temp_dict['title'] = data['title']\n","#       temp_dict['context'] = paragraph['context']\n","#       temp_dict['label'] = True\n","#       i+=1\n","#       print(i)\n","#       datas.append(temp_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWWPxnDp3EZV"},"outputs":[],"source":["# with open('/content/mlqa_vi_zalo_format.json', 'w',encoding='utf-8') as outfile:\n","#      json.dump(datas, outfile,ensure_ascii=False,indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCLJY9hOiGiR"},"outputs":[],"source":["# convert xquad format to zalo format\n","# xquad_vi_dict = {}\n","# datas = []\n","# i = 0\n","# for data in xquad_vi_df['data']:\n","#   for paragraph in data['paragraphs']:\n","#     for qas in paragraph['qas']:\n","#       temp_dict = {}\n","#       temp_dict['id'] = qas['id']\n","#       temp_dict['question'] = qas['question']\n","#       temp_dict['title'] = data['title']\n","#       temp_dict['context'] = paragraph['context']\n","#       temp_dict['label'] = True\n","      \n","#       datas.append(temp_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkPaOOqzqvMX"},"outputs":[],"source":["# with open('/content/xquad_vi_zalo_format.json', 'w',encoding='utf-8') as outfile:\n","#     json.dump(datas, outfile,ensure_ascii=False,indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSQWc9k-5o0r"},"outputs":[],"source":["# chia tập test lấy từ bộ dữ liệu của zalo\n","# with open('/content/drive/MyDrive/Fine-Tune-Bert/data/testing_zalodata.json', 'w') as file:\n","#     zalodata_df[0:3000].to_json(file, orient='records',force_ascii=False,indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_8Kp-Uz6iN4"},"outputs":[],"source":["# with open('/content/drive/MyDrive/Fine-Tune-Bert/data/trainingdata_zalo.json', 'w') as file:\n","#     zalodata_df[3000:].to_json(file, orient='records',force_ascii=False,indent=4)"]},{"cell_type":"markdown","metadata":{"id":"lpf8egIX-ywg"},"source":["#Train"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"vJTCOX3D9AIv","executionInfo":{"status":"ok","timestamp":1641891117123,"user_tz":-420,"elapsed":311,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["mlqadata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/mlqa_vi_zalo_format.json')\n","xquaddata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/xquad_vi_zaloformat_training_data.json')\n","zalodata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/trainingdata_zalo.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKXJw6nA91uH"},"outputs":[],"source":["zalodata_df = zalodata_df.rename({'text':'context'},axis =1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdJ0KwQN9OwR"},"outputs":[],"source":["trainingdata = zalodata_df#pd.concat((zalodata_df,mlqadata_df,xquaddata_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K11Tdcce9a5H","outputId":"cc868f25-1f39-46de-c88d-c87c3c3e20de","executionInfo":{"status":"ok","timestamp":1641886495531,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(15108, 5)"]},"metadata":{},"execution_count":7}],"source":["trainingdata.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHtUtdEYO3WU"},"outputs":[],"source":["cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ITKUNKWQhSr"},"outputs":[],"source":["columns = [\"context\", 'question','label']\n","data = []\n","for index, row in trainingdata.iterrows():\n","  temp = []\n","  temp.append(row['context'])\n","  temp.append(row['question'])\n","  temp.append(int(row['label']))\n","  data.append(temp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGxftmPlUbew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641886511364,"user_tz":-420,"elapsed":4089,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"4289e67c-f2cc-4e76-e1a7-af356ea13be5"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["phobert = AutoModel.from_pretrained(\"vinai/phobert-base\",output_hidden_states=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Twy0-O1T_EN"},"outputs":[],"source":["data = pd.DataFrame(data, columns=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLUksfdUElZk"},"outputs":[],"source":["data = data.sample(frac = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__WZDjD6MEKB"},"outputs":[],"source":["class TextDataset(Dataset):\n","  def __init__(self, text, question, label, tokenizer, max_length):\n","    super(TextDataset, self).__init__()\n","    self.tokenizer = tokenizer\n","    self.text = text\n","    self.question = question\n","    self.label = label\n","    self.max_length = max_length\n","  def __len__(self):\n","    return len(self.label)\n","  def __getitem__(self, index):\n","    inputs = self.tokenizer.encode_plus(\n","        self.question[index],self.text[index],\n","        pad_to_max_length = True,\n","        add_special_tokens = True,\n","        return_attention_mask = True,\n","        max_length = self.max_length,\n","        truncation = True,\n","    )\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","\n","    return{\n","        \"ids\" : torch.tensor(ids, dtype = torch.long, device = cuda),\n","        \"attention_mask\" : torch.tensor(mask, dtype = torch.long, device=cuda),\n","        \"target\" : torch.tensor(self.label[index], dtype = torch.long, device=cuda),\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BQixoR_WCc8"},"outputs":[],"source":["dataset = TextDataset(data['context'],data['question'],data['label'],tokenizer, 256)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POz_cfRDWsW6"},"outputs":[],"source":["datatrainloader = DataLoader(dataset=dataset, batch_size=16)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"H5xUn2Q1dJ1J","executionInfo":{"status":"ok","timestamp":1641891138615,"user_tz":-420,"elapsed":372,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["class BERT(nn.Module):\n","  def __init__(self, bert):\n","    super(BERT, self).__init__()\n","    self.bert = bert\n","    self.linear_1 = nn.Linear(768*4, 2)\n","    # self.sigmoid = nn.Sigmoid()\n","  def forward(self, input_ids, attention_mask):\n","    output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n","    cls_output = torch.cat((output[2][-1][:,0,...],\n","                            output[2][-2][:,0,...],\n","                            output[2][-3][:,0,...],\n","                            output[2][-4][:,0,...]), dim = -1)\n","    output = self.linear_1(cls_output)\n","    # output = self.sigmoid(output)\n","    return output"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"lcJjpbgzUfZM","executionInfo":{"status":"ok","timestamp":1641891140655,"user_tz":-420,"elapsed":339,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["model = BERT(phobert).to(cuda)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 5e-6)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"1le3NcRjVNyr","executionInfo":{"status":"ok","timestamp":1641891141009,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["for param in model.bert.parameters():\n","  param.requires_grad = False"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"oysHJGEpdsXF","executionInfo":{"status":"ok","timestamp":1641891141988,"user_tz":-420,"elapsed":2,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["transformers.logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzkQJj5YVRxi","outputId":"d94b7acf-ef67-4d9b-beb0-4e865985aa87","executionInfo":{"status":"ok","timestamp":1641893535132,"user_tz":-420,"elapsed":2392728,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/945 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["2\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["3\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["4\n"]},{"output_type":"stream","name":"stderr","text":[""]}],"source":["model.train()\n","for epoch in range(5):\n","  print(epoch)\n","  loop = tqdm(enumerate(datatrainloader), leave=False, total=len(datatrainloader))\n","  for batch, dl in loop:\n","    ids = dl['ids']\n","    attention_mask = dl['attention_mask']\n","    label = dl['target']\n","\n","    optimizer.zero_grad()\n","    output = model(input_ids=ids, attention_mask = attention_mask)\n","    # print(output)\n","    # print(label)\n","    # temp = torch.tensor(one_hot(label, 2),dtype=torch.float, device=cuda)\n","    loss = loss_function(output, label)\n","\n","    predict =torch.argmax(output, dim = 1)\n","    correct = sum(1 for a, b in zip(predict, label) if a == b)\n","    samples = output.shape[0]\n","    accuracy = correct/samples\n","    loss.backward()\n","    optimizer.step()\n","\n","    loop.set_description(f'Epoch={epoch}/10')\n","    loop.set_postfix(loss=loss.item(),acc=accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtGujByP1tDf"},"outputs":[],"source":["# torch.save(model.state_dict(), '/content/drive/MyDrive/Fine-Tune-Bert/model_QA_4CLS_phobert_mlqa_zalo_xquad_5epoch.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VR_tScSO10rx"},"outputs":[],"source":["# model = BERT(phobert).to(cuda)\n","# model.load_state_dict(torch.load('/content/drive/MyDrive/Fine-Tune-Bert/4_CLS/model_QA_4CLS_phobert_mlqa_zalo_xquad_5epoch.pth'))"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"IMz3WNYrg_32","executionInfo":{"status":"ok","timestamp":1641894069976,"user_tz":-420,"elapsed":303,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["def accuracy(datatrainloader,model):\n","  loop = tqdm(enumerate(datatrainloader), leave=False, total=len(datatrainloader))\n","  samples = 0\n","  correct = 0\n","  for batch, dl in loop:\n","    ids = dl['ids']\n","    attention_mask = dl['attention_mask']\n","    label = dl['target']\n","\n","    output = model(input_ids=ids, attention_mask = attention_mask)\n","    predict =torch.argmax(output, dim = 1)\n","\n","    temp_correct = sum(1 for a, b in zip(predict, label) if a == b)\n","    temp_samples = output.shape[0]\n","    samples += temp_samples\n","    correct += temp_correct\n","    accuracy = temp_correct/temp_samples\n","\n","    loop.set_postfix(acc=accuracy)\n","  return correct*1.0/samples"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txdcdSwyhzad","outputId":"ff434cbd-6ffe-4348-efeb-0d4e8eeba564","executionInfo":{"status":"ok","timestamp":1641894546737,"user_tz":-420,"elapsed":473915,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/945 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","                                                           "]},{"output_type":"stream","name":"stdout","text":["0.7011517077045274\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["print(accuracy(datatrainloader,model))"]},{"cell_type":"markdown","metadata":{"id":"wUBnMcqjkVEI"},"source":["#Evaluate in Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkla4QqTkS0o"},"outputs":[],"source":["# model = BERT(phobert).to(cuda)\n","# model.load_state_dict(torch.load('/content/drive/MyDrive/Fine-Tune-Bert/4_CLS/model_QA_4CLS_phobert_mlqa_zalo_xquad_5epoch.pth'))"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"aicssTp6kl1g","executionInfo":{"status":"ok","timestamp":1641894663826,"user_tz":-420,"elapsed":451,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["testset = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/testing_zalodata.json')"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b00tQk2Oktu_","outputId":"53dcc268-e980-4b09-c625-d53a81d5a4d3","executionInfo":{"status":"ok","timestamp":1641894664121,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 5)"]},"metadata":{},"execution_count":41}],"source":["testset.shape"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Bb10XCS2k13g","executionInfo":{"status":"ok","timestamp":1641894664933,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["testset = testset.rename({'text':'context'}, axis = 1)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"U2t4k0RSlHqd","executionInfo":{"status":"ok","timestamp":1641894665538,"user_tz":-420,"elapsed":311,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["columns = [\"context\", 'question','label']\n","data = []\n","for index, row in testset.iterrows():\n","  temp = []\n","  temp.append(row['context'])\n","  temp.append(row['question'])\n","  temp.append(int(row['label']))\n","  data.append(temp)\n","data = pd.DataFrame(data, columns=columns)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"Ic9GtlcYllT-","executionInfo":{"status":"ok","timestamp":1641894666677,"user_tz":-420,"elapsed":4,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["tx = TextDataset(data['context'][0:2000],data['question'][0:2000],data['label'][0:2000],tokenizer, 256)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"QA5KFq6wlOyV","executionInfo":{"status":"ok","timestamp":1641894666678,"user_tz":-420,"elapsed":4,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["dl = DataLoader(dataset=tx, batch_size=128)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"at1dl0CDlVvz","outputId":"0a0ef613-f2f3-4015-b56c-89005523f72a","executionInfo":{"status":"ok","timestamp":1641894728920,"user_tz":-420,"elapsed":61472,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","                                                          "]},{"output_type":"stream","name":"stdout","text":["0.616\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["print(accuracy(dl,model))"]},{"cell_type":"markdown","metadata":{"id":"HROuQMJF5kyM"},"source":["#Demo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-Lb8otb5hDO","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1641876768629,"user_tz":-420,"elapsed":779,"user":{"displayName":"Tuyển Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgL_7fwmi-6wsXMnaiF7SDMv2ievunYgF-1pGos=s64","userId":"04424011376780897176"}},"outputId":"caf2b1df-f58e-49a8-e5ea-323b3f3ecca8"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-ca4129184fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphobert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Models outputs are now tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m         )\n\u001b[1;32m    851\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"]}],"source":["line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n","\n","input_ids = torch.tensor([tokenizer.encode(line)])\n","\n","with torch.no_grad():\n","    features = phobert(input_ids)  # Models outputs are now tuples\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVzETepH5hDO"},"outputs":[],"source":["features.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkjrOCNx5hDO"},"outputs":[],"source":["features['hidden_states'][-1][:,0,:].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLh5JMgl5hDO"},"outputs":[],"source":["torch.cat((features['hidden_states'][-1][:,0,:],\n","           features['hidden_states'][-2][:,0,:],\n","           features['hidden_states'][-3][:,0,:],\n","           features['hidden_states'][-4][:,0,:]), dim = -1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuxkqWVFZIUu"},"outputs":[],"source":["features.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtnmDSlSl2Iy"},"outputs":[],"source":["features['hidden_states'][-1][:,0,:].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK3sNKOZTqxM"},"outputs":[],"source":["torch.cat((features['hidden_states'][-1][:,0,:],\n","           features['hidden_states'][-2][:,0,:],\n","           features['hidden_states'][-3][:,0,:],\n","           features['hidden_states'][-4][:,0,:]), dim = -1).shape"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"question_answer.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}