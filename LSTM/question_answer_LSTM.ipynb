{"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xhj4uxsZAEVC","executionInfo":{"status":"ok","timestamp":1641869077696,"user_tz":-420,"elapsed":8805,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"ab5f856d-33cc-40b0-e717-85d3ba849f0c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 36.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 475 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 63.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"]}]},{"cell_type":"code","execution_count":86,"metadata":{"id":"GfcsBLBugsRr","executionInfo":{"status":"ok","timestamp":1641872167297,"user_tz":-420,"elapsed":618,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import transformers\n","from transformers import AutoModel, AutoTokenizer\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import json\n","import tqdm"]},{"cell_type":"code","source":["phobert = AutoModel.from_pretrained(\"vinai/phobert-base\",output_hidden_states=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234,"referenced_widgets":["f744e6395f9046ffa1e693ddc116d011","9c8316ef1db54217a17ab8f50935ca23","b2c0e5e0fb7b4d8e8f40c54feb4636d8","3ac33e9454c64a93b980a2d0f61aba15","4d329391a5734940847242258ca59961","69d75c43513e484e8fb968aa206648f0","55171b30450248e2adf577ba33d64758","8091fcfefa934fb0bc7fe0435516780d","c33f52fa20bc4aba93889b5140db9578","dc78580bfd1f4680b401345c38c5e271","e518e41d991f489bacbd4378984c83b3","876217f58fa34218959fc833c5a52d8f","201a47993acb42f1a1bf4d1a845c8dc9","eeb10d64275646b08d9ab0d8d8fa7051","6feff9e68914404cba905892fef40dd7","4a1c9a2f1af9440c9c8dca739b458f79","52a627c88053430691ea76ef2f152159","338754f46a3b408e816da43c3254f481","740ebbf63f344e3a9b2e370a5210e637","2cfda2eb099d4d8c97dcd3075e2bbbe6","8979c85653f14ebaadf8ed6cbf716b9b","87ec3931824f482faca653c6ad530777","6938249add624082beb6dda5ec140fcd","44e874f098a14cc798720f5c3cdcd454","ad0500737928445199e8ed0ede6bfb43","215b110df4ed4de09886922f48f66c00","bbd39d9aa900465da84f162d0e73f3df","84f3a3cb1c714c5f9f940e48e6709005","251ee4d162344c148005fb3b1b9928f3","2039339718ac457f85351581782c16cb","073c91364ac84b09b8000e4e16f8b73e","860a17246e26487dbad9025a0a22d153","8c5fc5d889f7481b9e810420405e4eb1","963c302c1d644267a7f12b836d4a85f1","a395e13fabd648dbad8d5ff6fbfa4daf","9baf5a0b974e4262b61d9505435ab3ff","e2d83ffdcbd0401598ffca3f65b8b754","084fb3f730cd4a1e9c72077a969b6479","0587cfc2e35e47c5b2b4a05b459c4717","71adbf5ca0a1444b8cba73267b160416","6146be9759f34152874fad9144db3903","e32e49f2655c4eafa16d24349cceab4a","69ad5774190c4d279c9d70f9ee22b498","e99f9f1b01994c0b9bcbc56db4eaaf9d"]},"id":"3cdMkTylAL0Y","executionInfo":{"status":"ok","timestamp":1641869118031,"user_tz":-420,"elapsed":34480,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"ddf5e5f8-3c1e-4851-b16e-a5013f112463"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f744e6395f9046ffa1e693ddc116d011","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"876217f58fa34218959fc833c5a52d8f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6938249add624082beb6dda5ec140fcd","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/874k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"963c302c1d644267a7f12b836d4a85f1","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"markdown","source":["#Demo"],"metadata":{"id":"HROuQMJF5kyM"}},{"cell_type":"code","execution_count":29,"metadata":{"id":"Q-Lb8otb5hDO","executionInfo":{"status":"ok","timestamp":1641870213513,"user_tz":-420,"elapsed":480,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["line = \"Tôi là sinh_viên trường đại_học Công_nghệ .\"\n","\n","token = tokenizer.encode_plus(line,line)"]},{"cell_type":"code","source":["token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj38JJ4mF0ax","executionInfo":{"status":"ok","timestamp":1641870219410,"user_tz":-420,"elapsed":4,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"a10741d9-6049-425b-c012-62a71b740c37"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["with torch.no_grad():\n","    features = phobert(input_ids = torch.tensor([token['input_ids']]))  # Models outputs are now tuples"],"metadata":{"id":"jwINOebkFvYb","executionInfo":{"status":"ok","timestamp":1641870261879,"user_tz":-420,"elapsed":468,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["features['last_hidden_state'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmFqluKyGA6C","executionInfo":{"status":"ok","timestamp":1641870301222,"user_tz":-420,"elapsed":464,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"ea1624fa-a578-4c88-884d-0b21e4b3baa9"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 18, 768])"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["tokenizer.convert_ids_to_tokens(token['input_ids'])"],"metadata":{"id":"j4s525dsAwSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token['input_ids']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdwBcEMuMAoQ","executionInfo":{"status":"ok","timestamp":1641871840064,"user_tz":-420,"elapsed":501,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"73749020-f9b8-46cf-9dea-1b9ce829f441"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2]"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["token['input_ids'][0:token['input_ids'].index(2)+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGO6knpmGbTY","executionInfo":{"status":"ok","timestamp":1641870455716,"user_tz":-420,"elapsed":470,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"d8a4ecc8-bef6-4d9e-ea95-627c72e66cc5"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 218, 8, 649, 212, 956, 2413, 5, 2]"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["temp = torch.tensor([[[0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2],\n","                     [0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2],\n","                     [0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2]],\n","                     [[0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2],\n","                     [0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2],\n","                     [0, 218, 8, 649, 212, 956, 2413, 5, 2, 2, 218, 8, 649, 212, 956, 2413, 5, 2]]])"],"metadata":{"id":"3KrgymwCG6gp","executionInfo":{"status":"ok","timestamp":1641872079150,"user_tz":-420,"elapsed":510,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["temp.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lewpljoeM8I0","executionInfo":{"status":"ok","timestamp":1641872087816,"user_tz":-420,"elapsed":487,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"12298bdf-ae81-412e-d767-c7a2043e0e60"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 18])"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["temp_list = temp.tolist()"],"metadata":{"id":"ZULS6h_rHoMy","executionInfo":{"status":"ok","timestamp":1641870899554,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["result = []\n","for i in temp_list:\n","  temp = []\n","  for j in i:\n","    index = j.index(2)\n","    temp.append(j[:index+1])\n","  result.append(temp)"],"metadata":{"id":"OFoD8rspHrv2","executionInfo":{"status":"ok","timestamp":1641870902673,"user_tz":-420,"elapsed":777,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"WVzETepH5hDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641869587461,"user_tz":-420,"elapsed":480,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"983622c9-b77b-4167-dc05-9a1879a313f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"]},"metadata":{},"execution_count":14}],"source":["features.keys()"]},{"cell_type":"markdown","source":["#Train"],"metadata":{"id":"bP6lp_NP_9dN"}},{"cell_type":"code","source":["mlqadata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/mlqa_vi_zalo_format.json')\n","xquaddata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/xquad_vi_zaloformat_training_data.json')\n","zalodata_df = pd.read_json('/content/drive/MyDrive/Fine-Tune-Bert/data/trainingdata_zalo.json')"],"metadata":{"id":"JRd4q1AANUTY","executionInfo":{"status":"ok","timestamp":1641872223676,"user_tz":-420,"elapsed":5481,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["zalodata_df = zalodata_df.rename({'text':'context'},axis =1)\n","trainingdata = pd.concat((zalodata_df,mlqadata_df,xquaddata_df))"],"metadata":{"id":"_TwB9xYsNhyP","executionInfo":{"status":"ok","timestamp":1641872248116,"user_tz":-420,"elapsed":459,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"SooTESPCNkHM","executionInfo":{"status":"ok","timestamp":1641872250037,"user_tz":-420,"elapsed":2,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["columns = [\"context\", 'question','label']\n","data = []\n","for index, row in trainingdata.iterrows():\n","  temp = []\n","  temp.append(row['context'])\n","  temp.append(row['question'])\n","  temp.append(int(row['label']))\n","  data.append(temp)"],"metadata":{"id":"hVWy7KeGNmgQ","executionInfo":{"status":"ok","timestamp":1641872258620,"user_tz":-420,"elapsed":1581,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["phobert = AutoModel.from_pretrained(\"vinai/phobert-base\",output_hidden_states=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zkiy88FMNnnb","executionInfo":{"status":"ok","timestamp":1641872276741,"user_tz":-420,"elapsed":15211,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"81c4bc83-1753-43ff-d3cf-9f5fc5a5c112"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["data = pd.DataFrame(data, columns=columns)\n","data = data.sample(frac = 1)"],"metadata":{"id":"R5dk2OBwNreb","executionInfo":{"status":"ok","timestamp":1641872282320,"user_tz":-420,"elapsed":460,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","  def __init__(self, text, question, label, tokenizer, max_length):\n","    super(TextDataset, self).__init__()\n","    self.tokenizer = tokenizer\n","    self.text = text\n","    self.question = question\n","    self.label = label\n","    self.max_length = max_length\n","  def __len__(self):\n","    return len(self.label)\n","  def __getitem__(self, index):\n","    inputs = self.tokenizer.encode_plus(\n","        self.question[index],self.text[index],\n","        pad_to_max_length = True,\n","        add_special_tokens = True,\n","        return_attention_mask = True,\n","        max_length = self.max_length,\n","        truncation = True,\n","    )\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","\n","    return{\n","        \"ids\" : torch.tensor(ids, dtype = torch.long, device = cuda),\n","        \"attention_mask\" : torch.tensor(mask, dtype = torch.long, device=cuda),\n","        \"target\" : torch.tensor(self.label[index], dtype = torch.long, device=cuda),\n","    }"],"metadata":{"id":"513ThLl2NNhx","executionInfo":{"status":"ok","timestamp":1641872173534,"user_tz":-420,"elapsed":453,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["dataset = TextDataset(data['context'],data['question'],data['label'],tokenizer, 256)\n","datatrainloader = DataLoader(dataset=dataset, batch_size=16)"],"metadata":{"id":"fOuSdR8dNwng","executionInfo":{"status":"ok","timestamp":1641872303155,"user_tz":-420,"elapsed":467,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","  def __init__(self, input_size, hidden_size):\n","    super(LSTM, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","\n","    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n","  def forward(self, input):\n","    output,_ = self.lstm(input)\n","    return output"],"metadata":{"id":"KCQysCqSBVB1","executionInfo":{"status":"ok","timestamp":1641872489470,"user_tz":-420,"elapsed":481,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["ids.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kc-PtYFQTdB","executionInfo":{"status":"ok","timestamp":1641872971720,"user_tz":-420,"elapsed":440,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"bfd35e4d-2375-4f16-9ecc-99a8b948ca3e"},"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 256])"]},"metadata":{},"execution_count":122}]},{"cell_type":"code","source":["print(ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ODkFj18QyyV","executionInfo":{"status":"ok","timestamp":1641873106101,"user_tz":-420,"elapsed":879,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"42f53893-bbdf-48e7-d279-8fe8d54f9018"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    0, 11598,  3078,  ...,     1,     1,     1],\n","        [    0, 11598,  3078,  ...,     1,     1,     1],\n","        [    0,  1597,  2654,  ...,     1,     1,     1],\n","        ...,\n","        [    0,   319,   964,  ...,     1,     1,     1],\n","        [    0,   319,   504,  ...,     1,     1,     1],\n","        [    0,  3333,  4138,  ...,     1,     1,     1]])\n"]}]},{"cell_type":"code","source":["temp_list = ids.tolist()\n","result = []\n","for i in temp_list:\n","  temp = []\n","  temp.append(i[:i.index(2)+1] +[0]*(30-i.index(2)-1))\n","  result.append(temp)"],"metadata":{"id":"gZNyzaRqQXKc","executionInfo":{"status":"ok","timestamp":1641873142633,"user_tz":-420,"elapsed":440,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["ids[0].tolist().index(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5tX_Y38RSCi","executionInfo":{"status":"ok","timestamp":1641873238237,"user_tz":-420,"elapsed":449,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"6008d221-b076-4f9a-98ef-1bb1e543cd70"},"execution_count":138,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["print(np.array(result).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A5MpBDrsQ_uR","executionInfo":{"status":"ok","timestamp":1641873201335,"user_tz":-420,"elapsed":452,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"96c4458a-c86f-4a19-a00e-ba78e377924c"},"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["(16, 1)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","source":["def getQuestionVector(input, max_question_length):\n","  temp_list = input.tolist()\n","  result = []\n","  for i in temp_list:\n","    temp.append(i[:i.index(2)+1] +[0]*(max_question_length-i.index(2)-1))\n","    result.append(temp)\n","  return torch.tensor(result)"],"metadata":{"id":"3KH0_WwYJEtW","executionInfo":{"status":"ok","timestamp":1641872751569,"user_tz":-420,"elapsed":441,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["def getTextVector(input, max_text_length, output):\n","  temp_list = input.tolist()\n","  result = []\n","  for i in temp_list:\n","    temp = []\n","    for j in i:\n","      temp.append(j[j.index(2)+1:] +[0]*(max_text_length-len(j[j.index(2)+1:])))\n","    result.append(temp)\n","\n","  return torch.tensor(result)"],"metadata":{"id":"YFlfwhn3JbD6","executionInfo":{"status":"ok","timestamp":1641871705558,"user_tz":-420,"elapsed":587,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","execution_count":106,"metadata":{"id":"Sa4Jhi7YgsRv","executionInfo":{"status":"ok","timestamp":1641872492478,"user_tz":-420,"elapsed":3,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"outputs":[],"source":["class BERT(nn.Module):\n","  def __init__(self, bert):\n","    super(BERT, self).__init__()\n","    self.bert = bert\n","    self.linear_1 = nn.Linear(768*4, 2)\n","    self.lstm_question = LSTM(input_size=768, hidden_size= 30)\n","    self.lstm_context = LSTM(input_size=768, hidden_size= 256)\n","    \n","  def forward(self, input_ids, attention_mask):\n","    output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n","    print(output['last_hidden_state'].shape)\n","    cls_output = torch.cat((output['hidden_states'][-1][:,0,:],\n","                            output['hidden_states'][-2][:,0,:],\n","                            output['hidden_states'][-3][:,0,:],\n","                            output['hidden_states'][-4][:,0,:]), dim = -1)\n","    question = self.lstm_question(output['last_hidden_state'])\n","\n","    output = self.linear_1(cls_output)\n","    output = self.linear_2(output)\n","    # output = self.sigmoid(output)\n","    return output"]},{"cell_type":"code","source":["model = BERT(phobert).to(cuda)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 1e-6)"],"metadata":{"id":"3oS6x-ziN0-w","executionInfo":{"status":"ok","timestamp":1641872494929,"user_tz":-420,"elapsed":464,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["for param in model.bert.parameters():\n","  param.requires_grad = False\n","transformers.logging.set_verbosity_error()"],"metadata":{"id":"k_for3YjOjT3","executionInfo":{"status":"ok","timestamp":1641872512099,"user_tz":-420,"elapsed":482,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["ids[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2444ikyTPn-O","executionInfo":{"status":"ok","timestamp":1641872837056,"user_tz":-420,"elapsed":452,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"d2a4607e-d0b5-4356-e9b2-2cda62a9e874"},"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([256])"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["getQuestionVector(ids,64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"id":"c7W8WIfNPKFd","executionInfo":{"status":"error","timestamp":1641872755301,"user_tz":-420,"elapsed":6,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"326e7dbd-df08-4fad-890d-2fec66549816"},"execution_count":117,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-117-1826c8a08253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetQuestionVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-116-5634b562e225>\u001b[0m in \u001b[0;36mgetQuestionVector\u001b[0;34m(input, max_question_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_question_length\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"]}]},{"cell_type":"code","source":["ids.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4_7cnWwPDCB","executionInfo":{"status":"ok","timestamp":1641872644057,"user_tz":-420,"elapsed":926,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"851984e7-d672-4a7d-ca9c-1dccf5fbf3af"},"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 256])"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["model.train()\n","for epoch in range(5):\n","  print(epoch)\n","  loop = tqdm.tqdm(enumerate(datatrainloader), leave=False, total=len(datatrainloader))\n","  for batch, dl in loop:\n","    ids = dl['ids']\n","    attention_mask = dl['attention_mask']\n","    label = dl['target']\n","    print(ids.shape)\n","\n","    optimizer.zero_grad()\n","    output = model(input_ids=ids, attention_mask = attention_mask)\n","    # print(output)\n","    # print(label)\n","    # temp = torch.tensor(one_hot(label, 2),dtype=torch.float, device=cuda)\n","    loss = loss_function(output, label)\n","\n","    predict =torch.argmax(output, dim = 1)\n","    correct = sum(1 for a, b in zip(predict, label) if a == b)\n","    samples = output.shape[0]\n","    accuracy = correct/samples\n","    loss.backward()\n","    optimizer.step()\n","\n","    loop.set_description(f'Epoch={epoch}/10')\n","    loop.set_postfix(loss=loss.item(),acc=accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"qqhsUOeiOl_f","executionInfo":{"status":"error","timestamp":1641872604618,"user_tz":-420,"elapsed":11811,"user":{"displayName":"tuyen duong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02536347841230629858"}},"outputId":"c999b2bb-d09d-47c9-a8b3-356a64f158b2"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/992 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([16, 256])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-73c2645ca616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# print(label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-106-231232d860a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     cls_output = torch.cat((output['hidden_states'][-1][:,0,:],\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m                 )\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         )\n\u001b[1;32m    456\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"interpreter":{"hash":"d1405479f6767a4430f7f812a2e33ccedcbb91df1823cf9534b70b5d5031d199"},"kernelspec":{"display_name":"Python 3.7.1 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"orig_nbformat":4,"colab":{"name":"question_answer_LSTM.ipynb","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"f744e6395f9046ffa1e693ddc116d011":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9c8316ef1db54217a17ab8f50935ca23","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b2c0e5e0fb7b4d8e8f40c54feb4636d8","IPY_MODEL_3ac33e9454c64a93b980a2d0f61aba15","IPY_MODEL_4d329391a5734940847242258ca59961"]}},"9c8316ef1db54217a17ab8f50935ca23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b2c0e5e0fb7b4d8e8f40c54feb4636d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_69d75c43513e484e8fb968aa206648f0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55171b30450248e2adf577ba33d64758"}},"3ac33e9454c64a93b980a2d0f61aba15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8091fcfefa934fb0bc7fe0435516780d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":557,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":557,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c33f52fa20bc4aba93889b5140db9578"}},"4d329391a5734940847242258ca59961":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dc78580bfd1f4680b401345c38c5e271","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 557/557 [00:00&lt;00:00, 14.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e518e41d991f489bacbd4378984c83b3"}},"69d75c43513e484e8fb968aa206648f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55171b30450248e2adf577ba33d64758":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8091fcfefa934fb0bc7fe0435516780d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c33f52fa20bc4aba93889b5140db9578":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dc78580bfd1f4680b401345c38c5e271":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e518e41d991f489bacbd4378984c83b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"876217f58fa34218959fc833c5a52d8f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_201a47993acb42f1a1bf4d1a845c8dc9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_eeb10d64275646b08d9ab0d8d8fa7051","IPY_MODEL_6feff9e68914404cba905892fef40dd7","IPY_MODEL_4a1c9a2f1af9440c9c8dca739b458f79"]}},"201a47993acb42f1a1bf4d1a845c8dc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eeb10d64275646b08d9ab0d8d8fa7051":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52a627c88053430691ea76ef2f152159","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_338754f46a3b408e816da43c3254f481"}},"6feff9e68914404cba905892fef40dd7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_740ebbf63f344e3a9b2e370a5210e637","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":542923308,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":542923308,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2cfda2eb099d4d8c97dcd3075e2bbbe6"}},"4a1c9a2f1af9440c9c8dca739b458f79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8979c85653f14ebaadf8ed6cbf716b9b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 518M/518M [00:14&lt;00:00, 43.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87ec3931824f482faca653c6ad530777"}},"52a627c88053430691ea76ef2f152159":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"338754f46a3b408e816da43c3254f481":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"740ebbf63f344e3a9b2e370a5210e637":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2cfda2eb099d4d8c97dcd3075e2bbbe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8979c85653f14ebaadf8ed6cbf716b9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"87ec3931824f482faca653c6ad530777":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6938249add624082beb6dda5ec140fcd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_44e874f098a14cc798720f5c3cdcd454","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad0500737928445199e8ed0ede6bfb43","IPY_MODEL_215b110df4ed4de09886922f48f66c00","IPY_MODEL_bbd39d9aa900465da84f162d0e73f3df"]}},"44e874f098a14cc798720f5c3cdcd454":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad0500737928445199e8ed0ede6bfb43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_84f3a3cb1c714c5f9f940e48e6709005","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_251ee4d162344c148005fb3b1b9928f3"}},"215b110df4ed4de09886922f48f66c00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2039339718ac457f85351581782c16cb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":895321,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":895321,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_073c91364ac84b09b8000e4e16f8b73e"}},"bbd39d9aa900465da84f162d0e73f3df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_860a17246e26487dbad9025a0a22d153","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 874k/874k [00:01&lt;00:00, 1.05MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c5fc5d889f7481b9e810420405e4eb1"}},"84f3a3cb1c714c5f9f940e48e6709005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"251ee4d162344c148005fb3b1b9928f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2039339718ac457f85351581782c16cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"073c91364ac84b09b8000e4e16f8b73e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"860a17246e26487dbad9025a0a22d153":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8c5fc5d889f7481b9e810420405e4eb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"963c302c1d644267a7f12b836d4a85f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a395e13fabd648dbad8d5ff6fbfa4daf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9baf5a0b974e4262b61d9505435ab3ff","IPY_MODEL_e2d83ffdcbd0401598ffca3f65b8b754","IPY_MODEL_084fb3f730cd4a1e9c72077a969b6479"]}},"a395e13fabd648dbad8d5ff6fbfa4daf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9baf5a0b974e4262b61d9505435ab3ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0587cfc2e35e47c5b2b4a05b459c4717","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71adbf5ca0a1444b8cba73267b160416"}},"e2d83ffdcbd0401598ffca3f65b8b754":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6146be9759f34152874fad9144db3903","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1135173,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1135173,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e32e49f2655c4eafa16d24349cceab4a"}},"084fb3f730cd4a1e9c72077a969b6479":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_69ad5774190c4d279c9d70f9ee22b498","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.08M/1.08M [00:01&lt;00:00, 1.17MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e99f9f1b01994c0b9bcbc56db4eaaf9d"}},"0587cfc2e35e47c5b2b4a05b459c4717":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"71adbf5ca0a1444b8cba73267b160416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6146be9759f34152874fad9144db3903":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e32e49f2655c4eafa16d24349cceab4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69ad5774190c4d279c9d70f9ee22b498":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e99f9f1b01994c0b9bcbc56db4eaaf9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}